---
permalink: /
title: "Siteng Huang 黄思腾"
excerpt: "Siteng Huang 黄思腾"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Hi! I am Siteng Huang (黄思腾 in Chinese). I am a fifth-year Ph.D. student at [Zhejiang University](http://www.zju.edu.cn/), advised by Prof. [Donglin Wang](https://en.westlake.edu.cn/about/faculty/201912/t20191206_2513.shtml). Additionally, I am involved in a joint program with [Westlake University](https://www.westlake.edu.cn/) as a member of [Machine Intelligence Laboratory (MiLAB)](https://milab.westlake.edu.cn/). Currently, I am also a research intern at Alibaba Group. Prior to my Ph.D. career, I received my B.Eng. Degree from School of Computer Science, [Wuhan University](https://www.whu.edu.cn/) in 2019.

<!-- I am <span style="color:red;"><b>seeking exciting postdoc opportunities at top-tier institutions after Ph.D. graduation (July 2024)</b></span>. Please feel free to drop me an <a href="mailto:siteng.huang@gmail.com" target="_blank">email</a> if you are interested! -->

<h2 id='research-interests'>Research Interests</h2>

<!-- I am interested in technologies that allow machines and robots to learn like humans. In particular, I am committed to giving robots the ability to understand the world and learn from previous experiences, so that they can complete new tasks, acquire new skills or adapt to new environments rapidly with fewer samples through learning algorithms. Currently, my areas of interest include meta-learning, multi-task learning, and transfer learning on few/zero-shot learning tasks. I am also interested in deep learning, computer vision, and multimodal machine learning. -->

Currently, My research has centered on **multi-modal large models** (mainly on **vision-language models**), including 

* **Generation/AIGC**: text-to-image/video (T2I/V) generation, customized & controllable generation, diffusion models
* **Understanding**: text-video retrieval (TVR), compositional zero-shot learning (CZSL), few-shot learning (FSL), visual grounding
* **Adaptation**: parameter-efficient fine-tuning (PEFT/PETL), meta-learning, transfer learning
* **Embodied AI**: foundation models for robotics

I am always looking for related collaborations, and some of them have produced top-level publications. If you are interested in communication, feel free to drop me an <a href="mailto:siteng.huang@gmail.com" target="_blank">email</a>.

<!-- language-augmented vision -->

<!-- In the longer term, I am more concerned about

* giving robots the ability to understand the world and learn from previous experiences, so that they can complete new tasks, acquire new skills or adapt to new environments rapidly with fewer samples through learning algorithms. -->

<!-- 
1. 快速迁移，尤其是大模型
2. 机器人的主动学习，感知智能与行为智能 embodied
3. 开放世界
-->

<!-- I am interested in technologies that allow machines and robots to learn like humans. In particular, I am committed to giving robots the ability to understand the world and learn from previous experiences, so that they can complete new tasks, acquire new skills or adapt to new environments rapidly with fewer samples through learning algorithms.  -->

<h2 id='news'>News</h2>

* **[November 29, 2023]** A new [paper](https://arxiv.org/abs/2403.14520) about Cobra, an efficient multi-modal large language model, was released. [Project page](https://sites.google.com/view/cobravlm) has been available. The paper has been featured by [Hugging Face Daily Papers](https://huggingface.co/papers?date=2024-03-22)! [Demo](https://huggingface.co/spaces/han1997/cobra) has been available!
* **[March 13, 2024]** One paper about parameter-efficient tuning for visual grounding got accepted for ICME 2024.
* **[February 27, 2024]** Awarded as Zhejiang University 2024 Outstanding Graduates!
* **[February 27, 2024]** Three papers as first/co-first author got accepted for CVPR 2024. Congratulations to all collaborators!
* **[December 13, 2023]** One [paper](https://arxiv.org/abs/2309.01141) on diffusion model-based zero-shot visual grounding got accepted for ICASSP 2024. Congratulations to all collaborators!
* **[December 9, 2023]** One [paper](https://arxiv.org/abs/2312.09553) on VLM-based unsupervised domain adaptation got accepted for AAAI 2024.
* **[November 29, 2023]** A new [paper](https://arxiv.org/abs/2311.15841) on action-customized text-to-image generation was released. [Project page](https://adi-t2i.github.io/ADI/) has been available.
* **[November 29, 2023]** A new [paper](https://arxiv.org/abs/2311.15773) about a training-free layout calibration system for text-to-image generation was released. [Project page](https://simm-t2i.github.io/SimM/) has been available.
* **[July 24, 2023]** [2023 Scholar Metrics](https://scholar.googleblog.com/2023/07/2023-scholar-metrics-released.html) was released by Google Scholar. Our paper "[DSANet: Dual Self-Attention Network for Multivariate Time Series Forecasting](https://kyonhuang.top/publication/dual-self-attention-network)" ranked **8th** of the CIKM 2019 conference according to the citations, and **[26th](https://scholar.google.com/citations?hl=zh-CN&oe=GB&view_op=list_hcore&venue=V-IMg2OTpU8J.2023&vq=eng_databasesinformationsystems&cstart=20)** within five years.
* **[April 2, 2023]** [One paper about reference-limited compositional learning](https://kyonhuang.top/publication/reference-limited-CZSL) got accepted for ICMR 2023. Congratulations to all collaborators!
* **[February 28, 2023]** [One paper about parameter-efficient text-video retrieval](https://kyonhuang.top/publication/text-video-cooperative-prompt-tuning) got accepted for CVPR 2023. Congratulations to all collaborators!
<!-- * **[July 4, 2022]** One paper got accepted for ECCV 2022.  -->
<!-- * **[March 14, 2022]** Started as a research intern at DAMO Academy, Alibaba Group.  -->

<!-- **Service**: Always open to paper review, talk and organizing opportunities. Feel free to reach out to me if you are interested. -->
<!-- {: .notice--info} -->

<!-- Always open to research interns, cooperation and review opportunities. Feel free to reach out to me if you are interested. My email address is `huangsiteng [at] westlake.edu.cn`.
{: .notice--info} -->

<!-- **Hiring**: We are looking for **postdoctors, research assistants and visiting students for MiLAB in Westlake University** (currently only for Chinese). More information about requirements can be found [here](https://milab.westlake.edu.cn/contact.html), and if you are still in school, being a visiting student is also welcome. Please send email to `mi_lab[AT]westlake.edu.cn` with your CV if you are interested. Specially, if you are interested in my research direction and would like to be my collaborator after coming, please specify in the email and also send a copy to me.
{: .notice--info} -->

<h2 id='publications'>Publications</h2>

<a href="https://scholar.google.com/citations?user=mhpkWSYAAAAJ" target="_blank"><img src="https://img.shields.io/badge/dynamic/json?label=Paper%20Citations&query=total_citations&url=https%3A%2F%2Fcse.bth.se%2F~fer%2Fgooglescholar-api%2Fgooglescholar.php%3Fuser%3DmhpkWSYAAAAJ&logo=googlescholar&style=social" alt="Google Scholar"></a>  †: Equal contribution

### Peer-reviewed Conference

<img src="https://img.shields.io/badge/ICME-2024-blue?style=flat-square"> Ting Liu, Xuyang Liu, <u>Siteng Huang</u>, Honggang Chen, Quanjun Yin, Long Qin, Donglin Wang, Yue Hu, &quot;**DARA: Domain- and Relation-aware Adapters Make Parameter-efficient Tuning for Visual Grounding**&quot;. In *Proceedings of the IEEE Conference on Multimedia Expo 2024*.

<img src="https://img.shields.io/badge/CVPR-2024-blue?style=flat-square"> <u>Siteng Huang</u>, Biao Gong, Yutong Feng, Xi Chen, Yuqian Fu, Yu Liu, Donglin Wang, &quot;**Learning Disentangled Identifiers for Action-Customized Text-to-Image Generation**&quot;. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 2024*. [[arXiv](https://arxiv.org/abs/2311.15841)] [[project page](https://adi-t2i.github.io/ADI/)]

<img src="https://img.shields.io/badge/CVPR-2024-blue?style=flat-square"> Biao Gong†, <u>Siteng Huang</u>†, Yutong Feng, Shiwei Zhang, Yuyuan Li, Yu Liu, &quot;**Check, Locate, Rectify: A Training-Free Layout Calibration System for Text-to-Image Generation**&quot;. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 2024*. [[arXiv](https://arxiv.org/abs/2311.15773)] [[project page](https://simm-t2i.github.io/SimM/)]

<img src="https://img.shields.io/badge/CVPR-2024-blue?style=flat-square"> <u>Siteng Huang</u>, Biao Gong, Yutong Feng, Min Zhang, Yiliang Lv, Donglin Wang, &quot;**Troika: Multi-Path Cross-Modal Traction for Compositional Zero-Shot Learning**&quot;. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 2024*. [[arXiv](https://arxiv.org/abs/2303.15230)] [[project page](https://kyonhuang.top/publication/Troika)] [[github](https://github.com/bighuang624/Troika)]

<a href="https://ieeexplore.ieee.org/document/10445945" target="_blank"><img src="https://img.shields.io/badge/ICASSP-2024-blue?style=flat-square"></a> Xuyang Liu†, <u>Siteng Huang</u>†, Yachen Kang, Honggang Chen, Donglin Wang, &quot;**VGDiffZero: Text-to-image Diffusion Models Can Be Zero-shot Visual Grounders**&quot;. In *Proceedings of the 2024 IEEE International Conference on Acoustics, Speech and Signal Processing*. [[arXiv](https://arxiv.org/abs/2309.01141)] [[code](https://github.com/xuyang-liu16/VGDiffZero)]

<img src="https://img.shields.io/badge/AAAI-2024-blue?style=flat-square"> Shuanghao Bai, Min Zhang, Wanqi Zhou, <u>Siteng Huang</u>, Zhirong Luan, Donglin Wang, Badong Chen, &quot;**Prompt-based Distribution Alignment for Unsupervised Domain Adaptation**&quot;. In *Proceedings of the 38th AAAI Conference on Artificial Intelligence*. [[arXiv](https://arxiv.org/abs/2312.09553)]

<a href="https://ieeexplore.ieee.org/document/10203679" target="_blank"><img src="https://img.shields.io/badge/CVPR-2023-blue?style=flat-square"></a> <u>Siteng Huang</u>, Biao Gong, Yulin Pan, Jianwen Jiang, Yiliang Lv, Yuyuan Li, Donglin Wang, &quot;**VoP: Text-Video Co-operative Prompt Tuning for Cross-Modal Retrieval**&quot;. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 2023*. [[project page](https://kyonhuang.top/publication/text-video-cooperative-prompt-tuning)] [[arXiv](https://arxiv.org/abs/2211.12764)] [[open access](https://openaccess.thecvf.com/content/CVPR2023/html/Huang_VoP_Text-Video_Co-Operative_Prompt_Tuning_for_Cross-Modal_Retrieval_CVPR_2023_paper.html)] [[video (Youtube)](https://www.youtube.com/watch?v=ymdkiSSuOmI)] [[github](https://github.com/bighuang624/VoP)] [[ModelScope](https://modelscope.cn/models/damo/cv_vit-b32_retrieval_vop/summary)] [[poster](https://kyonhuang.top/files/VoP/CVPR23-VoP-poster.pdf)] [[slide](https://kyonhuang.top/files/VoP/CVPR23-VoP-presentation.pdf)] <a href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=mhpkWSYAAAAJ:W7OEmFMy1HYC" target="_blank"><img src="https://img.shields.io/badge/dynamic/json?label=citations&query=publications.4.citations&url=https%3A%2F%2Fcse.bth.se%2F~fer%2Fgooglescholar-api%2Fgooglescholar.php%3Fuser%3DmhpkWSYAAAAJ&logo=googlescholar&style=social"></a> <a href="https://github.com/bighuang624/VoP" target="_blank"><img src="https://img.shields.io/github/stars/bighuang624/VoP?style=social"></a>

<a href="https://doi.org/10.1145/3591106.3592225" target="_blank"><img src="https://img.shields.io/badge/ICMR-2023-blue?style=flat-square"></a> <u>Siteng Huang</u>, Qiyao Wei, Donglin Wang, &quot;**Reference-Limited Compositional Zero-Shot Learning**&quot;. In *Proceedings of the 2023 ACM International Conference on Multimedia Retrieval*. [[project page](https://kyonhuang.top/publication/reference-limited-CZSL)] [[arXiv](https://arxiv.org/abs/2208.10046)] [[video (Google Drive)](https://drive.google.com/file/d/1_wE_zbyvuGil_LrkmumotkRTLJJEUfCm/view?usp=drive_link)] [[github](https://github.com/bighuang624/RL-CZSL)] [[slide](https://kyonhuang.top/files/RLCZSL/ICMR23-RLCZSL-presentation.pdf)]

<a href="https://link.springer.com/chapter/10.1007/978-3-031-20044-1_26" target="_blank"><img src="https://img.shields.io/badge/ECCV-2022-blue?style=flat-square"></a> Min Zhang, <u>Siteng Huang</u>, Wenbin Li, Donglin Wang, &quot;**Tree Structure-Aware Few-Shot Image Classification via Hierarchical Aggregation**&quot;. In *Proceedings of the European Conference on Computer Vision 2022*. [[arXiv](https://arxiv.org/abs/2207.06989)] [[Chinese intro](https://zhuanlan.zhihu.com/p/543878686)] [[github](https://github.com/remiMZ/HTS-ECCV22)] <a href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=mhpkWSYAAAAJ:Tyk-4Ss8FVUC" target="_blank"><img src="https://img.shields.io/badge/dynamic/json?label=citations&query=publications.3.citations&url=https%3A%2F%2Fcse.bth.se%2F~fer%2Fgooglescholar-api%2Fgooglescholar.php%3Fuser%3DmhpkWSYAAAAJ&logo=googlescholar&style=social"></a>

<a href="https://ieeexplore.ieee.org/abstract/document/9747620" target="_blank"><img src="https://img.shields.io/badge/ICASSP-2022-blue?style=flat-square"></a> Min Zhang, <u>Siteng Huang</u>, Donglin Wang, &quot;**Domain Generalized Few-shot Image Classification via Meta Regularization Network**&quot;. In *Proceedings of the 2022 IEEE International Conference on Acoustics, Speech and Signal Processing*. [[pdf](https://kyonhuang.top/files/MRN/ICASSP22-MRN.pdf)] [[github](https://github.com/remiMZ/MRN-ICASSP22)]

<a href="https://dl.acm.org/doi/10.1145/3460426.3463614" target="_blank"><img src="https://img.shields.io/badge/ICMR-2021-blue?style=flat-square"></a> Zifeng Zhuang, Xintao Xiang, <u>Siteng Huang</u>, Donglin Wang, &quot;**HINFShot: A Challenge Dataset for Few-Shot Node Classification in Heterogeneous Information Network**&quot;. In *Proceedings of the 2021 ACM International Conference on Multimedia Retrieval*. [[pdf](https://kyonhuang.top/files/HINFShot/ICMR21-HINFShot.pdf)]

<a href="https://ieeexplore.ieee.org/document/9577454" target="_blank"><img src="https://img.shields.io/badge/CVPR-2021-blue?style=flat-square"></a> Zhengyu Chen, Jixie Ge, Heshen Zhan, <u>Siteng Huang</u>, Donglin Wang, &quot;**Pareto Self-Supervised Training for Few-Shot Learning**&quot;. In *Proceedings of the 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition*. [[arXiv](https://arxiv.org/abs/2104.07841)] [[open access](https://openaccess.thecvf.com/content/CVPR2021/html/Chen_Pareto_Self-Supervised_Training_for_Few-Shot_Learning_CVPR_2021_paper.html)] <a href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=mhpkWSYAAAAJ:2osOgNQ5qMEC" target="_blank"><img src="https://img.shields.io/badge/dynamic/json?label=citations&query=publications.1.citations&url=https%3A%2F%2Fcse.bth.se%2F~fer%2Fgooglescholar-api%2Fgooglescholar.php%3Fuser%3DmhpkWSYAAAAJ&logo=googlescholar&style=social"></a>

<a href="https://ojs.aaai.org/index.php/AAAI/article/view/16957" target="_blank"><img src="https://img.shields.io/badge/AAAI-2021-blue?style=flat-square"></a> <u>Siteng Huang</u>, Min Zhang, Yachen Kang, Donglin Wang, &quot;**Attributes-Guided and Pure-Visual Attention Alignment for Few-Shot Recognition**&quot;. In *Proceedings of the 35th AAAI Conference on Artificial Intelligence*. [[project page](https://kyonhuang.top/publication/attributes-guided-attention-module)] [[arXiv](https://arxiv.org/abs/2009.04724)] [[code](https://github.com/bighuang624/AGAM)] [[poster](https://kyonhuang.top/files/AGAM/aaai21-AGAM-poster.pdf)] [[slide](https://kyonhuang.top/files/AGAM/aaai21-AGAM-presentation.pdf)] <a href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=mhpkWSYAAAAJ:9yKSN-GCB0IC" target="_blank"><img src="https://img.shields.io/badge/dynamic/json?label=citations&query=publications.2.citations&url=https%3A%2F%2Fcse.bth.se%2F~fer%2Fgooglescholar-api%2Fgooglescholar.php%3Fuser%3DmhpkWSYAAAAJ&logo=googlescholar&style=social"></a> <a href="https://github.com/bighuang624/AGAM" target="_blank"><img src="https://img.shields.io/github/stars/bighuang624/AGAM?style=social"></a>

<a href="https://dl.acm.org/doi/abs/10.1145/3357384.3358132" target="_blank"><img src="https://img.shields.io/badge/CIKM-2019-blue?style=flat-square"></a> <u>Siteng Huang</u>, Donglin Wang, Xuehan Wu, Ao Tang, &quot;**DSANet: Dual Self-Attention Network for Multivariate Time Series Forecasting**&quot;. In *Proceedings of the 28th ACM International Conference on Information and Knowledge Management*. [[project page](https://kyonhuang.top/publication/dual-self-attention-network)] [[pdf](https://kyonhuang.top/files/DSANet/Huang-DSANet.pdf)] [[code](https://github.com/bighuang624/DSANet)] [[poster](https://kyonhuang.top/files/DSANet/cikm19-DSANet-poster.pdf)] [[slide](https://kyonhuang.top/files/DSANet/cikm19-DSANet-presentation.pdf)] <a href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=mhpkWSYAAAAJ:u-x6o8ySG0sC" target="_blank"><img src="https://img.shields.io/badge/dynamic/json?label=citations&query=publications.0.citations&url=https%3A%2F%2Fcse.bth.se%2F~fer%2Fgooglescholar-api%2Fgooglescholar.php%3Fuser%3DmhpkWSYAAAAJ&logo=googlescholar&style=social"></a> <a href="https://github.com/bighuang624/DSANet" target="_blank"><img src="https://img.shields.io/github/stars/bighuang624/DSANet?style=social"></a>

<!-- <a href="https://dl.acm.org/doi/abs/10.1145/3357384.3358132" style="text-decoration:none;"><span style="font-size:12px;color:#FFFFFF;background-color:#555555;padding:1px 4px 2px 6px;">CIKM</span><span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 6px 2px 4px;">2019</span></a> -->

### Preprints & Under Submission

<a href="https://arxiv.org/abs/2403.14520" target="_blank"><img src="https://img.shields.io/badge/arXiv-2403.14520-B31B1B?style=flat-square"></a> Han Zhao, Min Zhang, Wei Zhao, Pengxiang Ding, <u>Siteng Huang</u>, Donglin Wang, &quot;**Cobra: Extending Mamba to Multi-Modal Large Language Model for Efficient Inference**&quot;. *arXiv preprint arXiv:2403.14520*. [[pdf](https://arxiv.org/pdf/2403.14520.pdf)] [[project page](https://sites.google.com/view/cobravlm)] [[Chinese intro](https://zhuanlan.zhihu.com/p/688544752)] [[github](https://github.com/h-zhao1997/cobra)] [[demo](https://huggingface.co/spaces/han1997/cobra)] [[Twitter@AK](https://twitter.com/_akhaliq/status/1771033002748837953?t=6S4PVZXg6GcXqi_-PFzipw&s=19)]

<!-- ## Professional Experience -->

<h2 id='experience'>Experience</h2>

* Research Intern - **DAMO Academy/TongYi Lab, Alibaba Group (阿里巴巴达摩院/通义实验室)**
  * Time: March 2022 - Present.
  * [Fundamental Visual Intelligence Team](https://github.com/ali-vilab).

<!-- **Research Intern** | DAMO Academy, Alibaba Group | March 2022 - Present -->

<!-- * March 2022 - Present. *Research Intern*. <a href="https://damo.alibaba.com/" target="_blank">DAMO Academy, Alibaba Group</a>, Hangzhou, China. -->

<!-- <div style="float:left;">Research Intern</div><div style="float:right;">Mar. 2022 - Present</div>
<div style="float:left;"><a href="https://damo.alibaba.com/" target="_blank">DAMO Academy, Alibaba Group</a></div><div style="float:right;">Hangzhou, China</div> -->

<!-- <div>
<div style="float:left;">Research Intern<br><a href="https://damo.alibaba.com/" target="_blank">DAMO Academy, Alibaba Group</a></div><div style="float:right;">Mar. 2022 - Present<br>Hangzhou, China</div>
</div> -->

<h2 id='services'>Services</h2>

<!-- ### Journal Reviewer

* [IEEE Transactions on Neural Networks and Learning Systems (TNNLS)](https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385)

### Program Committee and/or Reviewer for Conferences and Workshops

* [ICCV 2023](https://iccv2023.thecvf.com/) -->

### Journal Reviewer

* IEEE Transactions on Neural Networks and Learning Systems [(TNNLS)](https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385)
* ACM Transactions on Intelligent Systems and Technology [(ACM TIST)](https://dl.acm.org/journal/tist)
* Concurrency and Computation: Practice and Experience [(CPE)](https://onlinelibrary.wiley.com/journal/15320634)

### Conference Reviewer

* IEEE/CVF Conference on Computer Vision and Pattern Recognition [(CVPR)](https://ieeexplore.ieee.org/xpl/conhome/1000147/all-proceedings)
* IEEE/CVF International Conference on Computer Vision [(ICCV)](https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings)
* European Conference on Computer Vision [(ECCV)](https://www.ecva.net/index.php#conferences)
* AAAI Conference on Artificial Intelligence [(AAAI)](https://aaai.org/conference/aaai/)
* International Joint Conference on Artificial Intelligence [(IJCAI)](https://www.ijcai.org/)
* IEEE International Conference on Multimedia and Expo [(ICME)](https://ieeexplore.ieee.org/xpl/conhome/1000477/all-proceedings)
* ACM International Conference on Multimedia Retrieval [(ICMR)](http://icmr2024.org/)
* Asian Conference on Computer Vision [(ACCV)](https://link.springer.com/conference/accv)
* International Conference on Pattern Recognition [(ICPR)](https://ieeexplore.ieee.org/xpl/conhome/1000545/all-proceedings)

### Program Committee for Conferences and Workshops

* Session Chair, The First Westlake Robot Learning Symposium

<h2 id='misc'>Misc</h2>

Welcome to follow my [Zhihu](https://www.zhihu.com/people/huang-si-teng-67) and [Chinese blog](https://kyonhuang.top/blog/).

<!-- <div align="middle">
  <a href="https://milab.westlake.edu.cn/" target="_blank"><img align="middle" style="max-width: 300px; width: 100%; margin-right: 40px; margin-top: 10px" src="https://kyonhuang.top/images/milab_logo.png" /></a>
  <a href="http://www.zju.edu.cn/" target="_blank"><img align="middle" style="max-width: 160px; width: 100%; margin-left: 20px; margin-top: 10px" src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/color-zju-logo.png" /></a>
</div> -->
