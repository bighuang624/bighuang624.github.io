# ðŸ“¢ News

<!-- å‚è€ƒ https://huanwang.tech/ çš„æ ·å¼ -->

* <span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 5px 1.5px 5px;">2025/05/07</span> **[Preprint]** We released [OpenHelix](https://arxiv.org/abs/2505.03912), a low-cost open-source dual-system VLA with systematic empirical evaluations on the core design elements. [Code](https://github.com/Cuixxx/OpenHelix) and [List of papers](https://github.com/OpenHelix-robot/awesome-dual-system-vla/) have been available!
* <span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 5px 1.5px 5px;">2025/03/31</span> **[Preprint]** We released [Unicorn](https://arxiv.org/abs/2503.22655) to explore the question: can high-quality multimodal training data be synthesized purely from text?
* <span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 5px 1.5px 5px;">2025/03/28</span> **[Survey Preprint]** We released [Exploring the Evolution of Physics Cognition in Video Generation: A Survey](https://arxiv.org/abs/2503.21765), which dives deep into the development of physics cognition in video generation, from basic perception to active cognition! [List of papers](https://github.com/minnie-lin/Awesome-Physics-Cognition-based-Video-Generation) has been available!
* <span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 5px 1.5px 5px;">2025/03/11</span> **[TCSVT'25]** [M2IST](https://arxiv.org/abs/2407.01131), a novel Multi-Modal Interactive Side-Tuning method that effectively addresses the challenges of insufficient multi-modal interaction and high GPU memory consumption, got accepted for IEEE Transactions on Circuits and Systems for Video Technology! [Code](https://github.com/xuyang-liu16/M2IST) has been available!
* <span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 5px 1.5px 5px;">2025/02/24</span> **[Preprint]** We released [Humanoid-VLA](https://arxiv.org/abs/2502.14795), a novel framework that integrates language understanding, egocentric scene perception, and motion control, enabling universal humanoid control!
* <span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 5px 1.5px 5px;">2025/01/28</span> **[ICRA'25]** [QUART-Online](https://arxiv.org/abs/2412.15576), a novel latency-free quadruped MLLM model that achieves real-time inference while boosting the success rate across various tasks by 65%, got accepted for ICRA 2025! See [Project page](https://quart-online.github.io/).
* <span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 5px 1.5px 5px;">2025/01/23</span> **[ICLR'25]** [ToCa](https://arxiv.org/abs/2410.05317), a token-wise feature caching method that achieves a 2x acceleration for PixArt-Î±, OpenSora, and DiT while maintaining nearly lossless generation quality, got accepted for ICLR 2025! [Code](https://github.com/Shenyi-Z/ToCa) has been available!
* <span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 5px 1.5px 5px;">2025/01/10</span> **[Preprint]** We released [GlobalCom<sup>2</sup>](https://arxiv.org/abs/2501.05179), a "global-to-local" approach for training-free acceleration of high-resolution MLLMs with AnyRes strategy. [Code](https://github.com/xuyang-liu16/GlobalCom2) has been available!
* <span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 5px 1.5px 5px;">2024/12/13</span> **[Preprint]** We released [Score and Distribution Matching Policy](https://arxiv.org/abs/2412.09265), which transforms diffusion-based policies into single-step generators for light-weight deployment, fast reasoning and precise action generation. [Project page](https://sdm-policy.github.io/) has been available.
* <span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 5px 1.5px 5px;">2024/12/10</span> **[Preprint]** We released [CARP](https://arxiv.org/abs/2412.06782), **C**oarse-to-fine **A**uto**R**egressive **P**rediction for visuomotor policy learning. The approach produces highly accurate and smooth robot actions, achieving up to a 10% improvement of success rates, and delivers 10x faster inference compared to state-of-the-art policies. [Project page](https://carp-robot.github.io/) with cool videos has been available. Code will be available soon!
* <span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 5px 1.5px 5px;">2024/12/10</span> **[AAAI'25]** [Cobra](https://arxiv.org/abs/2403.14520), the first Mamba-based MLLM with efficient inference, got accepted for AAAI 2025! See [Project page](https://sites.google.com/view/cobravlm).
* <span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 5px 1.5px 5px;">2024/11/27</span> **[Preprint]** We released a new work on **token reduction for MLLM inference acceleration**, [which](https://arxiv.org/abs/2411.17686) proposes a **unified paradigm** to demystify the popular works and guide the future designs, and further offers a suite of methods **FiCoCo** grounded in the paradigm. [Project page](https://FiCoCo-accelerate.github.io/) and [code](https://github.com/kawhiiiileo/FiCoCo) have been available!
* <span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 5px 1.5px 5px;">2024/09/09</span> **[New Start]** Joined Alibaba DAMO Academy as an Algorithm Expert!
* <span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 5px 1.5px 5px;">2024/07/16</span> **[MM'24]** One paper ([ProFD](https://openreview.net/forum?id=o2axlPlXYY)) got accepted for ACM MM 2024. Congratulations to all collaborators!
* <span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 5px 1.5px 5px;">2024/07/09</span> **[Scholar'24]** [2024 Scholar Metrics](https://scholar.googleblog.com/2024/07/2024-scholar-metrics-released.html) was released by Google Scholar. Our paper "[DSANet: Dual Self-Attention Network for Multivariate Time Series Forecasting](https://kyonhuang.top/publication/dual-self-attention-network)" ranked **7th** of the CIKM 2019 conference according to the citations, and **[13th](https://scholar.google.com/citations?hl=zh-CN&oe=GB&view_op=list_hcore&venue=V-IMg2OTpU8J.2024&vq=eng_databasesinformationsystems&cstart=0)** within five years.
* <span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 5px 1.5px 5px;">2024/07/01</span> **[ECCV'24]** Two papers ([PiTe](http://arxiv.org/abs/2409.07239) and [QUAR-VLA](https://arxiv.org/abs/2312.14457)) got accepted for ECCV 2024. <span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 5px 1.5px 5px;">2024/08/12</span> PiTe got accepted as an Oral paper!
* <span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 5px 1.5px 5px;">2024/06/04</span> **[Graduation]** I successfully defended my dissertation. So many thanks to my Ph.D. committee (Prof. [Xiaogang Jin](http://www.cad.zju.edu.cn/home/jin/), Prof. [Mai Xu](https://shi.buaa.edu.cn/xumai/en/index.htm), Prof. [Changxin Gao](http://faculty.hust.edu.cn/cgao/en/index.htm), Prof. [Fajie Yuan](https://en.westlake.edu.cn/faculty/fajie-yuan.html), Prof. [Peidong Liu](https://en.westlake.edu.cn/faculty/peidong-liu.html), Prof. [Xiaofei Li](https://en.westlake.edu.cn/faculty/xiaofei-li.html)) and my advisor!
* <span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 5px 1.5px 5px;">2024/03/29</span> **[VALSE'24]** [Troika](https://arxiv.org/abs/2303.15230) got accepted as [VALSE 2024](https://valser.org/2024/#/) Poster! <span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 5px 1.5px 5px;">2024/05/05</span> Our [Cobra](https://sites.google.com/view/cobravlm) was selected for [VALSE 2024](https://valser.org/2024/#/) [Annual Progress Representation](https://kyonhuang.top/files/Cobra/VALSE24-APR-Cobra.jpg). Thanks to all the committee for the approval!
<!-- * <span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 5px 1.5px 5px;">2024/03/21</span> **[Preprint]** [Cobra](https://arxiv.org/abs/2403.14520), an efficient multi-modal large language model, was released. [Project page](https://sites.google.com/view/cobravlm) has been available. The paper has been featured by [Hugging Face Daily Papers](https://huggingface.co/papers?date=2024-03-22)! [Demo](https://huggingface.co/spaces/han1997/cobra) has been available! <span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 5px 1.5px 5px;">2024/12/10</span> Cobra got accepted for AAAI 2025! -->
* <span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 5px 1.5px 5px;">2024/03/13</span> **[ICME'24]** One paper ([DARA](https://arxiv.org/abs/2405.06217)) about parameter-efficient tuning for visual grounding got accepted for ICME 2024 (Oral).
* <span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 5px 1.5px 5px;">2024/02/27</span> **[Award]** Awarded as Zhejiang University 2024 Outstanding Graduates!
* <span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 5px 1.5px 5px;">2024/02/27</span> **[CVPR'24]** Three papers (<a href="https://arxiv.org/abs/2311.15841" target="_blank">ADI</a>, <a href="https://arxiv.org/abs/2303.15230" target="_blank">Troika</a>, <a href="https://arxiv.org/abs/2311.15773" target="_blank">SimM</a>) as first/co-first author got accepted for CVPR 2024. Congratulations to all collaborators!
<!-- * <span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 5px 1.5px 5px;">2023/12/13</span> **[ICASSP'24]** One paper ([VGDiffZero](https://arxiv.org/abs/2309.01141)) on diffusion model-based zero-shot visual grounding got accepted for ICASSP 2024. Congratulations to all collaborators! -->
<!-- * <span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 5px 1.5px 5px;">2023/12/09</span> **[AAAI'24]** One [paper](https://arxiv.org/abs/2312.09553) on VLM-based unsupervised domain adaptation got accepted for AAAI 2024. -->
<!-- * <span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 5px 1.5px 5px;">2023/04/02</span> **[ICMR'23]** One paper ([RL-CZSL](https://kyonhuang.top/publication/reference-limited-CZSL)) about reference-limited compositional learning got accepted for ICMR 2023. Congratulations to all collaborators! -->
<!-- * <span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 5px 1.5px 5px;">2023/02/28</span> **[CVPR'23]** One paper ([VoP](https://kyonhuang.top/publication/text-video-cooperative-prompt-tuning)) about parameter-efficient text-video retrieval got accepted for CVPR 2023. Congratulations to all collaborators! -->