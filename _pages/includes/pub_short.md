# üìù Publications

‚Ä†: Equal contribution

### Peer-reviewed Conference

<a href="https://openreview.net/forum?id=o2axlPlXYY" target="_blank"><img src="https://img.shields.io/badge/ACMMM-2024-blue?style=flat-square"></a> Can Cui‚Ä†, <u>Siteng Huang</u>‚Ä†, Wenxuan Song, Pengxiang Ding, Zhang Min, Donglin Wang, &quot;**ProFD: Prompt-Guided Feature Disentangling for Occluded Person Re-Identification**&quot;. In *Proceedings of the ACM Multimedia 2024*. [[arXiv](https://arxiv.org/abs/2409.20081)] [[github](https://github.com/Cuixxx/ProFD)]

<img src="https://img.shields.io/badge/ECCV-2024-blue?style=flat-square"> <span style="color:red">(Oral)</span> Yang Liu‚Ä†, Pengxiang Ding‚Ä†, <u>Siteng Huang</u>, Min Zhang, Han Zhao, Donglin Wang, &quot;**PiTe: Pixel-Temporal Alignment for Large Video-Language Model**&quot;. In *Proceedings of the European Conference on Computer Vision 2024*. [[arXiv](http://arxiv.org/abs/2409.07239)] [[github](https://github.com/yliu-cs/PiTe)] [[dataset](https://yliu-cs.github.io/PiTe/)]

<img src="https://img.shields.io/badge/ECCV-2024-blue?style=flat-square"> Pengxiang Ding, Han Zhao, Wenxuan Song, Wenjie Zhang, Min Zhang, <u>Siteng Huang</u>, Ningxi Yang, Donglin Wang, &quot;**QUAR-VLA: Vision-Language-Action Model for Quadruped Robots**&quot;. In *Proceedings of the European Conference on Computer Vision 2024*. [[arXiv](https://arxiv.org/abs/2312.14457)]

<a href="https://ieeexplore.ieee.org/document/10688132" target="_blank"><img src="https://img.shields.io/badge/ICME-2024-blue?style=flat-square"></a> <span style="color:red">(Oral)</span> Ting Liu‚Ä†, Xuyang Liu‚Ä†, <u>Siteng Huang</u>, Honggang Chen, Quanjun Yin, Long Qin, Donglin Wang, Yue Hu, &quot;**DARA: Domain- and Relation-aware Adapters Make Parameter-efficient Tuning for Visual Grounding**&quot;. In *Proceedings of the IEEE Conference on Multimedia Expo 2024*. [[arXiv](https://arxiv.org/abs/2405.06217)] [[github](https://github.com/liuting20/DARA)]

<a href="https://ieeexplore.ieee.org/document/10658048" target="_blank"><img src="https://img.shields.io/badge/CVPR-2024-blue?style=flat-square"></a> <u>Siteng Huang</u>, Biao Gong, Yutong Feng, Xi Chen, Yuqian Fu, Yu Liu, Donglin Wang, &quot;**Learning Disentangled Identifiers for Action-Customized Text-to-Image Generation**&quot;. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 2024*. [[arXiv](https://arxiv.org/abs/2311.15841)] [[dataset](https://github.com/bighuang624/ActionBench)] [[project page](https://adi-t2i.github.io/ADI/)] [[poster (CVPR 2024)](https://kyonhuang.top/files/ADI/CVPR24-ADI-poster.pdf)] <a class='paper_citations_badges' data='mhpkWSYAAAAJ:_FxGoFyzp5QC' href="" target="_blank"></a>

<a href="https://ieeexplore.ieee.org/document/10657271" target="_blank"><img src="https://img.shields.io/badge/CVPR-2024-blue?style=flat-square"></a> Biao Gong‚Ä†, <u>Siteng Huang</u>‚Ä†, Yutong Feng, Shiwei Zhang, Yuyuan Li, Yu Liu, &quot;**Check, Locate, Rectify: A Training-Free Layout Calibration System for Text-to-Image Generation**&quot;. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 2024*. [[arXiv](https://arxiv.org/abs/2311.15773)] [[project page](https://simm-t2i.github.io/SimM/)] [[poster (CVPR 2024)](https://kyonhuang.top/files/SimM/CVPR24-SimM-poster.pdf)]

<a href="https://ieeexplore.ieee.org/document/10656633" target="_blank"><img src="https://img.shields.io/badge/CVPR-2024-blue?style=flat-square"></a> <u>Siteng Huang</u>, Biao Gong, Yutong Feng, Min Zhang, Yiliang Lv, Donglin Wang, &quot;**Troika: Multi-Path Cross-Modal Traction for Compositional Zero-Shot Learning**&quot;. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 2024*. [[arXiv](https://arxiv.org/abs/2303.15230)] [[project page](https://kyonhuang.top/publication/Troika)] [[github](https://github.com/bighuang624/Troika)] [[poster (CVPR 2024)](https://kyonhuang.top/files/Troika/CVPR24-Troika-poster.pdf)] [[poster (VALSE 2024)](https://kyonhuang.top/files/Troika/VALSE2024-Troika-Poster.pdf)] <a class='paper_citations_badges' data='mhpkWSYAAAAJ:eQOLeE2rZwMC' href="" target="_blank"></a>

<a href="https://ieeexplore.ieee.org/document/10445945" target="_blank"><img src="https://img.shields.io/badge/ICASSP-2024-blue?style=flat-square"></a> Xuyang Liu‚Ä†, <u>Siteng Huang</u>‚Ä†, Yachen Kang, Honggang Chen, Donglin Wang, &quot;**VGDiffZero: Text-to-image Diffusion Models Can Be Zero-shot Visual Grounders**&quot;. In *Proceedings of the 2024 IEEE International Conference on Acoustics, Speech and Signal Processing*. [[arXiv](https://arxiv.org/abs/2309.01141)] [[code](https://github.com/xuyang-liu16/VGDiffZero)] [[poster](https://kyonhuang.top/files/VGDiffZero/ICASSP2024-VGDiffZero-Poster.pdf)] <a class='paper_citations_badges' data='mhpkWSYAAAAJ:ufrVoPGSRksC' href="" target="_blank"></a>

<a href="https://ojs.aaai.org/index.php/AAAI/article/view/27830" target="_blank"><img src="https://img.shields.io/badge/AAAI-2024-blue?style=flat-square"></a> Shuanghao Bai, Min Zhang, Wanqi Zhou, <u>Siteng Huang</u>, Zhirong Luan, Donglin Wang, Badong Chen, &quot;**Prompt-based Distribution Alignment for Unsupervised Domain Adaptation**&quot;. In *Proceedings of the 38th AAAI Conference on Artificial Intelligence*. [[arXiv](https://arxiv.org/abs/2312.09553)] <a class='paper_citations_badges' data='mhpkWSYAAAAJ:roLk4NBRz8UC' href="" target="_blank"></a>

<a href="https://ieeexplore.ieee.org/document/10203679" target="_blank"><img src="https://img.shields.io/badge/CVPR-2023-blue?style=flat-square"></a> <u>Siteng Huang</u>, Biao Gong, Yulin Pan, Jianwen Jiang, Yiliang Lv, Yuyuan Li, Donglin Wang, &quot;**VoP: Text-Video Co-operative Prompt Tuning for Cross-Modal Retrieval**&quot;. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 2023*. [[project page](https://kyonhuang.top/publication/text-video-cooperative-prompt-tuning)] [[arXiv](https://arxiv.org/abs/2211.12764)] [[open access](https://openaccess.thecvf.com/content/CVPR2023/html/Huang_VoP_Text-Video_Co-Operative_Prompt_Tuning_for_Cross-Modal_Retrieval_CVPR_2023_paper.html)] [[video (Youtube)](https://www.youtube.com/watch?v=ymdkiSSuOmI)] [[github](https://github.com/bighuang624/VoP)] [[ModelScope](https://modelscope.cn/models/damo/cv_vit-b32_retrieval_vop/summary)] [[poster](https://kyonhuang.top/files/VoP/CVPR23-VoP-poster.pdf)] [[slide](https://kyonhuang.top/files/VoP/CVPR23-VoP-presentation.pdf)] <a class='paper_citations_badges' data='mhpkWSYAAAAJ:W7OEmFMy1HYC' href="" target="_blank"></a> <a href="https://github.com/bighuang624/VoP" target="_blank"><img src="https://img.shields.io/github/stars/bighuang624/VoP?style=social"></a>

<a href="https://doi.org/10.1145/3591106.3592225" target="_blank"><img src="https://img.shields.io/badge/ICMR-2023-blue?style=flat-square"></a> <u>Siteng Huang</u>, Qiyao Wei, Donglin Wang, &quot;**Reference-Limited Compositional Zero-Shot Learning**&quot;. In *Proceedings of the 2023 ACM International Conference on Multimedia Retrieval*. [[project page](https://kyonhuang.top/publication/reference-limited-CZSL)] [[arXiv](https://arxiv.org/abs/2208.10046)] [[video (Google Drive)](https://drive.google.com/file/d/1_wE_zbyvuGil_LrkmumotkRTLJJEUfCm/view?usp=drive_link)] [[github](https://github.com/bighuang624/RL-CZSL)] [[slide](https://kyonhuang.top/files/RLCZSL/ICMR23-RLCZSL-presentation.pdf)]

<a href="https://link.springer.com/chapter/10.1007/978-3-031-20044-1_26" target="_blank"><img src="https://img.shields.io/badge/ECCV-2022-blue?style=flat-square"></a> Min Zhang, <u>Siteng Huang</u>, Wenbin Li, Donglin Wang, &quot;**Tree Structure-Aware Few-Shot Image Classification via Hierarchical Aggregation**&quot;. In *Proceedings of the European Conference on Computer Vision 2022*. [[arXiv](https://arxiv.org/abs/2207.06989)] [[Chinese intro](https://zhuanlan.zhihu.com/p/543878686)] [[github](https://github.com/remiMZ/HTS-ECCV22)] <a class='paper_citations_badges' data='mhpkWSYAAAAJ:Tyk-4Ss8FVUC' href="" target="_blank"></a>

<a href="https://ieeexplore.ieee.org/abstract/document/9747620" target="_blank"><img src="https://img.shields.io/badge/ICASSP-2022-blue?style=flat-square"></a> Min Zhang, <u>Siteng Huang</u>, Donglin Wang, &quot;**Domain Generalized Few-shot Image Classification via Meta Regularization Network**&quot;. In *Proceedings of the 2022 IEEE International Conference on Acoustics, Speech and Signal Processing*. [[pdf](https://kyonhuang.top/files/MRN/ICASSP22-MRN.pdf)] [[github](https://github.com/remiMZ/MRN-ICASSP22)] <a class='paper_citations_badges' data='mhpkWSYAAAAJ:zYLM7Y9cAGgC' href="" target="_blank"></a>

<a href="https://dl.acm.org/doi/10.1145/3460426.3463614" target="_blank"><img src="https://img.shields.io/badge/ICMR-2021-blue?style=flat-square"></a> Zifeng Zhuang, Xintao Xiang, <u>Siteng Huang</u>, Donglin Wang, &quot;**HINFShot: A Challenge Dataset for Few-Shot Node Classification in Heterogeneous Information Network**&quot;. In *Proceedings of the 2021 ACM International Conference on Multimedia Retrieval*. [[pdf](https://kyonhuang.top/files/HINFShot/ICMR21-HINFShot.pdf)] <a class='paper_citations_badges' data='mhpkWSYAAAAJ:UeHWp8X0CEIC' href="" target="_blank"></a>

<a href="https://ieeexplore.ieee.org/document/9577454" target="_blank"><img src="https://img.shields.io/badge/CVPR-2021-blue?style=flat-square"></a> Zhengyu Chen, Jixie Ge, Heshen Zhan, <u>Siteng Huang</u>, Donglin Wang, &quot;**Pareto Self-Supervised Training for Few-Shot Learning**&quot;. In *Proceedings of the 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition*. [[arXiv](https://arxiv.org/abs/2104.07841)] [[open access](https://openaccess.thecvf.com/content/CVPR2021/html/Chen_Pareto_Self-Supervised_Training_for_Few-Shot_Learning_CVPR_2021_paper.html)] <a class='paper_citations_badges' data='mhpkWSYAAAAJ:2osOgNQ5qMEC' href="" target="_blank"></a>

<a href="https://ojs.aaai.org/index.php/AAAI/article/view/16957" target="_blank"><img src="https://img.shields.io/badge/AAAI-2021-blue?style=flat-square"></a> <u>Siteng Huang</u>, Min Zhang, Yachen Kang, Donglin Wang, &quot;**Attributes-Guided and Pure-Visual Attention Alignment for Few-Shot Recognition**&quot;. In *Proceedings of the 35th AAAI Conference on Artificial Intelligence*. [[project page](https://kyonhuang.top/publication/attributes-guided-attention-module)] [[arXiv](https://arxiv.org/abs/2009.04724)] [[code](https://github.com/bighuang624/AGAM)] [[poster](https://kyonhuang.top/files/AGAM/aaai21-AGAM-poster.pdf)] [[slide](https://kyonhuang.top/files/AGAM/aaai21-AGAM-presentation.pdf)] <a class='paper_citations_badges' data='mhpkWSYAAAAJ:9yKSN-GCB0IC' href="" target="_blank"></a> <a href="https://github.com/bighuang624/AGAM" target="_blank"><img src="https://img.shields.io/github/stars/bighuang624/AGAM?style=social"></a>

<a href="https://dl.acm.org/doi/abs/10.1145/3357384.3358132" target="_blank"><img src="https://img.shields.io/badge/CIKM-2019-blue?style=flat-square"></a> <u>Siteng Huang</u>, Donglin Wang, Xuehan Wu, Ao Tang, &quot;**DSANet: Dual Self-Attention Network for Multivariate Time Series Forecasting**&quot;. In *Proceedings of the 28th ACM International Conference on Information and Knowledge Management*. [[project page](https://kyonhuang.top/publication/dual-self-attention-network)] [[pdf](https://kyonhuang.top/files/DSANet/Huang-DSANet.pdf)] [[code](https://github.com/bighuang624/DSANet)] [[poster](https://kyonhuang.top/files/DSANet/cikm19-DSANet-poster.pdf)] [[slide](https://kyonhuang.top/files/DSANet/cikm19-DSANet-presentation.pdf)] <a class='paper_citations_badges' data='mhpkWSYAAAAJ:u-x6o8ySG0sC' href="" target="_blank"></a> <a href="https://github.com/bighuang624/DSANet" target="_blank"><img src="https://img.shields.io/github/stars/bighuang624/DSANet?style=social"></a>

<!-- <a href="https://dl.acm.org/doi/abs/10.1145/3357384.3358132" style="text-decoration:none;"><span style="font-size:12px;color:#FFFFFF;background-color:#555555;padding:1px 4px 2px 6px;">CIKM</span><span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 6px 2px 4px;">2019</span></a> -->

### Preprints & Under Submission

<a href="https://arxiv.org/abs/2410.05317" target="_blank"><img src="https://img.shields.io/badge/arXiv-2410.05317-B31B1B?style=flat-square"></a> Chang Zou‚Ä†, Xuyang Liu‚Ä†, Ting Liu, <u>Siteng Huang</u>, Linfeng Zhang, &quot;**Accelerating Diffusion Transformers with Token-wise Feature Caching**&quot;. *arXiv preprint arXiv:2410.05317*. [[pdf](https://arxiv.org/pdf/2410.05317.pdf)] [[github](https://github.com/Shenyi-Z/ToCa)]

<a href="https://arxiv.org/abs/2408.17083" target="_blank"><img src="https://img.shields.io/badge/arXiv-2408.17083-B31B1B?style=flat-square"></a> Fengyuan Dai, <u>Siteng Huang</u>, Min Zhang, Biao Gong, Donglin Wang, &quot;**Focus-Consistent Multi-Level Aggregation for Compositional Zero-Shot Learning**&quot;. *arXiv preprint arXiv:2408.17083*. [[pdf](https://arxiv.org/pdf/2408.17083.pdf)]

<a href="https://arxiv.org/abs/2407.01131" target="_blank"><img src="https://img.shields.io/badge/arXiv-2407.01131-B31B1B?style=flat-square"></a> Xuyang Liu‚Ä†, Ting Liu‚Ä†, <u>Siteng Huang</u>, Yue Hu, Quanjun Yin, Donglin Wang, Honggang Chen, &quot;**M<sup>2</sup>IST: Multi-Modal Interactive Side-Tuning for Memory-efficient Referring Expression Comprehension**&quot;. *arXiv preprint arXiv:2407.01131*. [[pdf](https://arxiv.org/pdf/2407.01131.pdf)]

<a href="https://arxiv.org/abs/2405.14700" target="_blank"><img src="https://img.shields.io/badge/arXiv-2405.14700-B31B1B?style=flat-square"></a> Ting Liu‚Ä†, Xuyang Liu‚Ä†, <u>Siteng Huang</u>, Liangtao Shi, Zunnan Xu, Yi Xin, Quanjun Yin, Xiaohong Liu, &quot;**Sparse-Tuning: Adapting Vision Transformers with Efficient Fine-tuning and Inference**&quot;. *arXiv preprint arXiv:2405.14700*. [[pdf](https://arxiv.org/pdf/2405.14700.pdf)] [[github](https://github.com/liuting20/Sparse-Tuning)]

<a href="https://arxiv.org/abs/2403.14520" target="_blank"><img src="https://img.shields.io/badge/arXiv-2403.14520-B31B1B?style=flat-square"></a> Han Zhao, Min Zhang, Wei Zhao, Pengxiang Ding, <u>Siteng Huang</u>, Donglin Wang, &quot;**Cobra: Extending Mamba to Multi-Modal Large Language Model for Efficient Inference**&quot;. *arXiv preprint arXiv:2403.14520*. [[pdf](https://arxiv.org/pdf/2403.14520.pdf)] [[project page](https://sites.google.com/view/cobravlm)] [[Chinese intro (Zhihu)](https://zhuanlan.zhihu.com/p/688544752)] [[github](https://github.com/h-zhao1997/cobra)] [[demo](https://huggingface.co/spaces/han1997/cobra)] [[video (Youtube)](https://www.youtube.com/watch?v=i0sTdi_yVbc)] [[Êú∫Âô®‰πãÂøÉ](https://mp.weixin.qq.com/s/KuuNTL_jBRsyhub5_6aXpQ)] [[Twitter@AK](https://twitter.com/_akhaliq/status/1771033002748837953?t=6S4PVZXg6GcXqi_-PFzipw&s=19)] <a class='paper_citations_badges' data='mhpkWSYAAAAJ:Se3iqnhoufwC' href="" target="_blank"></a> <a href="https://github.com/h-zhao1997/cobra" target="_blank"><img src="https://img.shields.io/github/stars/h-zhao1997/cobra?style=social"></a>