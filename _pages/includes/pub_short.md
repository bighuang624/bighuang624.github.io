# üìù Publications

‚Ä†: Equal contribution
‚úâ: Corresponding author

### Peer-reviewed Conference

<img src="https://img.shields.io/badge/ICRA-2025-blue?style=flat-square"> Xinyang Tong, Pengxiang Ding, Donglin Wang, Wenjie Zhang, Can Cui, Mingyang Sun, Yiguo Fan, Han Zhao, Hongyin Zhang, Yonghao Dang, <u>Siteng Huang</u>, Shangke Lyu, &quot;**QUART-Online: Latency-Free Large Multimodal Language Model for Quadruped Robot Learning**&quot;. In *Proceedings of the 2025 IEEE International Conference on Robotics and Automation*. [[arXiv](https://arxiv.org/abs/2412.15576)] [[project page](https://quart-online.github.io/)]

<img src="https://img.shields.io/badge/ICLR-2025-blue?style=flat-square"> Chang Zou‚Ä†, Xuyang Liu‚Ä†, Ting Liu, <u>Siteng Huang</u>, Linfeng Zhang, &quot;**Accelerating Diffusion Transformers with Token-wise Feature Caching**&quot;. In *Proceedings of the 13th International Conference on Learning Representations*. [[arXiv](https://arxiv.org/abs/2410.05317)] [[github](https://github.com/Shenyi-Z/ToCa)] <a class='paper_citations_badges' data='mhpkWSYAAAAJ:Zph67rFs4hoC' href="" target="_blank"></a>

<img src="https://img.shields.io/badge/AAAI-2025-blue?style=flat-square"> Han Zhao, Min Zhang, Wei Zhao, Pengxiang Ding, <u>Siteng Huang</u>, Donglin Wang, &quot;**Cobra: Extending Mamba to Multi-Modal Large Language Model for Efficient Inference**&quot;. In *Proceedings of the 39th AAAI Conference on Artificial Intelligence*. [[arXiv](https://arxiv.org/abs/2403.14520)] [[pdf](https://arxiv.org/pdf/2403.14520.pdf)] [[project page](https://sites.google.com/view/cobravlm)] [[Chinese intro (Zhihu)](https://zhuanlan.zhihu.com/p/688544752)] [[github](https://github.com/h-zhao1997/cobra)] [[demo](https://huggingface.co/spaces/han1997/cobra)] [[video (Youtube)](https://www.youtube.com/watch?v=i0sTdi_yVbc)] [[Êú∫Âô®‰πãÂøÉ](https://mp.weixin.qq.com/s/KuuNTL_jBRsyhub5_6aXpQ)] [[Twitter@AK](https://twitter.com/_akhaliq/status/1771033002748837953?t=6S4PVZXg6GcXqi_-PFzipw&s=19)] <a class='paper_citations_badges' data='mhpkWSYAAAAJ:Se3iqnhoufwC' href="" target="_blank"></a> <a href="https://github.com/h-zhao1997/cobra" target="_blank"><img src="https://img.shields.io/github/stars/h-zhao1997/cobra?style=social"></a>

<a href="https://dl.acm.org/doi/10.1145/3664647.3680958" target="_blank"><img src="https://img.shields.io/badge/ACMMM-2024-blue?style=flat-square"></a> Can Cui‚Ä†, <u>Siteng Huang</u>‚Ä†, Wenxuan Song, Pengxiang Ding, Zhang Min, Donglin Wang, &quot;**ProFD: Prompt-Guided Feature Disentangling for Occluded Person Re-Identification**&quot;. In *Proceedings of the 32nd ACM International Conference on Multimedia*. [[arXiv](https://arxiv.org/abs/2409.20081)] [[github](https://github.com/Cuixxx/ProFD)] [[OpenReview](https://openreview.net/forum?id=o2axlPlXYY)]

<a href="https://link.springer.com/chapter/10.1007/978-3-031-72652-1_10" target="_blank"><img src="https://img.shields.io/badge/ECCV-2024-blue?style=flat-square"></a> <span style="color:red">(Oral)</span> Yang Liu‚Ä†, Pengxiang Ding‚Ä†, <u>Siteng Huang</u>, Min Zhang, Han Zhao, Donglin Wang, &quot;**PiTe: Pixel-Temporal Alignment for Large Video-Language Model**&quot;. In *Proceedings of the European Conference on Computer Vision 2024*. [[arXiv](http://arxiv.org/abs/2409.07239)] [[github](https://github.com/yliu-cs/PiTe)] [[dataset](https://yliu-cs.github.io/PiTe/)]

<a href="https://link.springer.com/chapter/10.1007/978-3-031-72652-1_21" target="_blank"><img src="https://img.shields.io/badge/ECCV-2024-blue?style=flat-square"></a> Pengxiang Ding, Han Zhao, Wenxuan Song, Wenjie Zhang, Min Zhang, <u>Siteng Huang</u>, Ningxi Yang, Donglin Wang, &quot;**QUAR-VLA: Vision-Language-Action Model for Quadruped Robots**&quot;. In *Proceedings of the European Conference on Computer Vision 2024*. [[arXiv](https://arxiv.org/abs/2312.14457)] <a class='paper_citations_badges' data='mhpkWSYAAAAJ:3fE2CSJIrl8C' href="" target="_blank"></a>

<a href="https://ieeexplore.ieee.org/document/10688132" target="_blank"><img src="https://img.shields.io/badge/ICME-2024-blue?style=flat-square"></a> <span style="color:red">(Oral)</span> Ting Liu‚Ä†, Xuyang Liu‚Ä†, <u>Siteng Huang</u>, Honggang Chen, Quanjun Yin, Long Qin, Donglin Wang, Yue Hu, &quot;**DARA: Domain- and Relation-aware Adapters Make Parameter-efficient Tuning for Visual Grounding**&quot;. In *Proceedings of the IEEE Conference on Multimedia Expo 2024*. [[arXiv](https://arxiv.org/abs/2405.06217)] [[github](https://github.com/liuting20/DARA)] <a class='paper_citations_badges' data='mhpkWSYAAAAJ:UebtZRa9Y70C' href="" target="_blank"></a>

<a href="https://ieeexplore.ieee.org/document/10658048" target="_blank"><img src="https://img.shields.io/badge/CVPR-2024-blue?style=flat-square"></a> <u>Siteng Huang</u>, Biao Gong, Yutong Feng, Xi Chen, Yuqian Fu, Yu Liu, Donglin Wang, &quot;**Learning Disentangled Identifiers for Action-Customized Text-to-Image Generation**&quot;. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 2024*. [[arXiv](https://arxiv.org/abs/2311.15841)] [[dataset](https://github.com/bighuang624/ActionBench)] [[project page](https://adi-t2i.github.io/ADI/)] [[poster (CVPR 2024)](https://kyonhuang.top/files/ADI/CVPR24-ADI-poster.pdf)] <a class='paper_citations_badges' data='mhpkWSYAAAAJ:_FxGoFyzp5QC' href="" target="_blank"></a>

<a href="https://ieeexplore.ieee.org/document/10657271" target="_blank"><img src="https://img.shields.io/badge/CVPR-2024-blue?style=flat-square"></a> Biao Gong‚Ä†, <u>Siteng Huang</u>‚Ä†, Yutong Feng, Shiwei Zhang, Yuyuan Li, Yu Liu, &quot;**Check, Locate, Rectify: A Training-Free Layout Calibration System for Text-to-Image Generation**&quot;. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 2024*. [[arXiv](https://arxiv.org/abs/2311.15773)] [[project page](https://simm-t2i.github.io/SimM/)] [[poster (CVPR 2024)](https://kyonhuang.top/files/SimM/CVPR24-SimM-poster.pdf)] <a class='paper_citations_badges' data='mhpkWSYAAAAJ:LkGwnXOMwfcC' href="" target="_blank"></a>

<a href="https://ieeexplore.ieee.org/document/10656633" target="_blank"><img src="https://img.shields.io/badge/CVPR-2024-blue?style=flat-square"></a> <u>Siteng Huang</u>, Biao Gong, Yutong Feng, Min Zhang, Yiliang Lv, Donglin Wang, &quot;**Troika: Multi-Path Cross-Modal Traction for Compositional Zero-Shot Learning**&quot;. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 2024*. [[arXiv](https://arxiv.org/abs/2303.15230)] [[project page](https://kyonhuang.top/publication/Troika)] [[github](https://github.com/bighuang624/Troika)] [[poster (CVPR 2024)](https://kyonhuang.top/files/Troika/CVPR24-Troika-poster.pdf)] [[poster (VALSE 2024)](https://kyonhuang.top/files/Troika/VALSE2024-Troika-Poster.pdf)] <a class='paper_citations_badges' data='mhpkWSYAAAAJ:eQOLeE2rZwMC' href="" target="_blank"></a>

<a href="https://ieeexplore.ieee.org/document/10445945" target="_blank"><img src="https://img.shields.io/badge/ICASSP-2024-blue?style=flat-square"></a> Xuyang Liu‚Ä†, <u>Siteng Huang</u>‚Ä†, Yachen Kang, Honggang Chen, Donglin Wang, &quot;**VGDiffZero: Text-to-image Diffusion Models Can Be Zero-shot Visual Grounders**&quot;. In *Proceedings of the 2024 IEEE International Conference on Acoustics, Speech and Signal Processing*. [[arXiv](https://arxiv.org/abs/2309.01141)] [[code](https://github.com/xuyang-liu16/VGDiffZero)] [[poster](https://kyonhuang.top/files/VGDiffZero/ICASSP2024-VGDiffZero-Poster.pdf)] <a class='paper_citations_badges' data='mhpkWSYAAAAJ:ufrVoPGSRksC' href="" target="_blank"></a>

<a href="https://ojs.aaai.org/index.php/AAAI/article/view/27830" target="_blank"><img src="https://img.shields.io/badge/AAAI-2024-blue?style=flat-square"></a> Shuanghao Bai, Min Zhang, Wanqi Zhou, <u>Siteng Huang</u>, Zhirong Luan, Donglin Wang, Badong Chen, &quot;**Prompt-based Distribution Alignment for Unsupervised Domain Adaptation**&quot;. In *Proceedings of the 38th AAAI Conference on Artificial Intelligence*. [[arXiv](https://arxiv.org/abs/2312.09553)] <a class='paper_citations_badges' data='mhpkWSYAAAAJ:roLk4NBRz8UC' href="" target="_blank"></a>

<a href="https://ieeexplore.ieee.org/document/10203679" target="_blank"><img src="https://img.shields.io/badge/CVPR-2023-blue?style=flat-square"></a> <u>Siteng Huang</u>, Biao Gong, Yulin Pan, Jianwen Jiang, Yiliang Lv, Yuyuan Li, Donglin Wang, &quot;**VoP: Text-Video Co-operative Prompt Tuning for Cross-Modal Retrieval**&quot;. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 2023*. [[project page](https://kyonhuang.top/publication/text-video-cooperative-prompt-tuning)] [[arXiv](https://arxiv.org/abs/2211.12764)] [[open access](https://openaccess.thecvf.com/content/CVPR2023/html/Huang_VoP_Text-Video_Co-Operative_Prompt_Tuning_for_Cross-Modal_Retrieval_CVPR_2023_paper.html)] [[video (Youtube)](https://www.youtube.com/watch?v=ymdkiSSuOmI)] [[github](https://github.com/bighuang624/VoP)] [[ModelScope](https://modelscope.cn/models/damo/cv_vit-b32_retrieval_vop/summary)] [[poster](https://kyonhuang.top/files/VoP/CVPR23-VoP-poster.pdf)] [[slide](https://kyonhuang.top/files/VoP/CVPR23-VoP-presentation.pdf)] <a class='paper_citations_badges' data='mhpkWSYAAAAJ:W7OEmFMy1HYC' href="" target="_blank"></a> <a href="https://github.com/bighuang624/VoP" target="_blank"><img src="https://img.shields.io/github/stars/bighuang624/VoP?style=social"></a>

<a href="https://doi.org/10.1145/3591106.3592225" target="_blank"><img src="https://img.shields.io/badge/ICMR-2023-blue?style=flat-square"></a> <u>Siteng Huang</u>, Qiyao Wei, Donglin Wang, &quot;**Reference-Limited Compositional Zero-Shot Learning**&quot;. In *Proceedings of the 2023 ACM International Conference on Multimedia Retrieval*. [[project page](https://kyonhuang.top/publication/reference-limited-CZSL)] [[arXiv](https://arxiv.org/abs/2208.10046)] [[video (Google Drive)](https://drive.google.com/file/d/1_wE_zbyvuGil_LrkmumotkRTLJJEUfCm/view?usp=drive_link)] [[github](https://github.com/bighuang624/RL-CZSL)] [[slide](https://kyonhuang.top/files/RLCZSL/ICMR23-RLCZSL-presentation.pdf)] <a class='paper_citations_badges' data='mhpkWSYAAAAJ:Y0pCki6q_DkC' href="" target="_blank"></a>

<a href="https://link.springer.com/chapter/10.1007/978-3-031-20044-1_26" target="_blank"><img src="https://img.shields.io/badge/ECCV-2022-blue?style=flat-square"></a> Min Zhang, <u>Siteng Huang</u>, Wenbin Li, Donglin Wang, &quot;**Tree Structure-Aware Few-Shot Image Classification via Hierarchical Aggregation**&quot;. In *Proceedings of the European Conference on Computer Vision 2022*. [[arXiv](https://arxiv.org/abs/2207.06989)] [[Chinese intro](https://zhuanlan.zhihu.com/p/543878686)] [[github](https://github.com/remiMZ/HTS-ECCV22)] <a class='paper_citations_badges' data='mhpkWSYAAAAJ:Tyk-4Ss8FVUC' href="" target="_blank"></a>

<a href="https://ieeexplore.ieee.org/abstract/document/9747620" target="_blank"><img src="https://img.shields.io/badge/ICASSP-2022-blue?style=flat-square"></a> Min Zhang, <u>Siteng Huang</u>, Donglin Wang, &quot;**Domain Generalized Few-shot Image Classification via Meta Regularization Network**&quot;. In *Proceedings of the 2022 IEEE International Conference on Acoustics, Speech and Signal Processing*. [[pdf](https://kyonhuang.top/files/MRN/ICASSP22-MRN.pdf)] [[github](https://github.com/remiMZ/MRN-ICASSP22)] <a class='paper_citations_badges' data='mhpkWSYAAAAJ:zYLM7Y9cAGgC' href="" target="_blank"></a>

<a href="https://dl.acm.org/doi/10.1145/3460426.3463614" target="_blank"><img src="https://img.shields.io/badge/ICMR-2021-blue?style=flat-square"></a> Zifeng Zhuang, Xintao Xiang, <u>Siteng Huang</u>, Donglin Wang, &quot;**HINFShot: A Challenge Dataset for Few-Shot Node Classification in Heterogeneous Information Network**&quot;. In *Proceedings of the 2021 ACM International Conference on Multimedia Retrieval*. [[pdf](https://kyonhuang.top/files/HINFShot/ICMR21-HINFShot.pdf)] <a class='paper_citations_badges' data='mhpkWSYAAAAJ:UeHWp8X0CEIC' href="" target="_blank"></a>

<a href="https://ieeexplore.ieee.org/document/9577454" target="_blank"><img src="https://img.shields.io/badge/CVPR-2021-blue?style=flat-square"></a> Zhengyu Chen, Jixie Ge, Heshen Zhan, <u>Siteng Huang</u>, Donglin Wang, &quot;**Pareto Self-Supervised Training for Few-Shot Learning**&quot;. In *Proceedings of the 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition*. [[arXiv](https://arxiv.org/abs/2104.07841)] [[open access](https://openaccess.thecvf.com/content/CVPR2021/html/Chen_Pareto_Self-Supervised_Training_for_Few-Shot_Learning_CVPR_2021_paper.html)] <a class='paper_citations_badges' data='mhpkWSYAAAAJ:2osOgNQ5qMEC' href="" target="_blank"></a>

<a href="https://ojs.aaai.org/index.php/AAAI/article/view/16957" target="_blank"><img src="https://img.shields.io/badge/AAAI-2021-blue?style=flat-square"></a> <u>Siteng Huang</u>, Min Zhang, Yachen Kang, Donglin Wang, &quot;**Attributes-Guided and Pure-Visual Attention Alignment for Few-Shot Recognition**&quot;. In *Proceedings of the 35th AAAI Conference on Artificial Intelligence*. [[project page](https://kyonhuang.top/publication/attributes-guided-attention-module)] [[arXiv](https://arxiv.org/abs/2009.04724)] [[code](https://github.com/bighuang624/AGAM)] [[poster](https://kyonhuang.top/files/AGAM/aaai21-AGAM-poster.pdf)] [[slide](https://kyonhuang.top/files/AGAM/aaai21-AGAM-presentation.pdf)] <a class='paper_citations_badges' data='mhpkWSYAAAAJ:9yKSN-GCB0IC' href="" target="_blank"></a> <a href="https://github.com/bighuang624/AGAM" target="_blank"><img src="https://img.shields.io/github/stars/bighuang624/AGAM?style=social"></a>

<a href="https://dl.acm.org/doi/abs/10.1145/3357384.3358132" target="_blank"><img src="https://img.shields.io/badge/CIKM-2019-blue?style=flat-square"></a> <u>Siteng Huang</u>, Donglin Wang, Xuehan Wu, Ao Tang, &quot;**DSANet: Dual Self-Attention Network for Multivariate Time Series Forecasting**&quot;. In *Proceedings of the 28th ACM International Conference on Information and Knowledge Management*. [[project page](https://kyonhuang.top/publication/dual-self-attention-network)] [[pdf](https://kyonhuang.top/files/DSANet/Huang-DSANet.pdf)] [[code](https://github.com/bighuang624/DSANet)] [[poster](https://kyonhuang.top/files/DSANet/cikm19-DSANet-poster.pdf)] [[slide](https://kyonhuang.top/files/DSANet/cikm19-DSANet-presentation.pdf)] <a class='paper_citations_badges' data='mhpkWSYAAAAJ:u-x6o8ySG0sC' href="" target="_blank"></a> <a href="https://github.com/bighuang624/DSANet" target="_blank"><img src="https://img.shields.io/github/stars/bighuang624/DSANet?style=social"></a>

<!-- <a href="https://dl.acm.org/doi/abs/10.1145/3357384.3358132" style="text-decoration:none;"><span style="font-size:12px;color:#FFFFFF;background-color:#555555;padding:1px 4px 2px 6px;">CIKM</span><span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 6px 2px 4px;">2019</span></a> -->

### Peer-reviewed Journal

<a href="https://ieeexplore.ieee.org/document/10929057" target="_blank"><img src="https://img.shields.io/badge/TCSVT-2025-49846a?style=flat-square"></a> Xuyang Liu‚Ä†, Ting Liu‚Ä†, <u>Siteng Huang‚úâ</u>, Yi Xin, Yue Hu, Long Qin, Donglin Wang, Honggang Chen‚úâ, &quot;**M2IST: Multi-Modal Interactive Side-Tuning for Efficient Referring Expression Comprehension**&quot;. *IEEE Transactions on Circuits and Systems for Video Technology*, 2025. [[arXiv](https://arxiv.org/abs/2407.01131)] [[github](https://github.com/xuyang-liu16/M2IST)] <a class='paper_citations_badges' data='mhpkWSYAAAAJ:kNdYIx-mwKoC' href="" target="_blank"></a>

### Preprints & Under Submission

<a href="https://arxiv.org/abs/2503.21765" target="_blank"><img src="https://img.shields.io/badge/arXiv-2503.21765-B31B1B?style=flat-square"></a> Minghui Lin, Xiang Wang, Yishan Wang, Shu Wang, Fengqi Dai, Pengxiang Ding, Cunxiang Wang, Zhengrong Zuo, Nong Sang, <u>Siteng Huang‚úâ</u>, and Donglin Wang‚úâ, &quot;**Exploring the Evolution of Physics Cognition in Video Generation: A Survey**&quot;. *arXiv preprint arXiv:2503.21765*. [[pdf](https://arxiv.org/pdf/2503.21765.pdf)] [[github](https://github.com/minnie-lin/Awesome-Physics-Cognition-based-Video-Generation)] [[huggingface paper](https://huggingface.co/papers/2503.21765)]

<a href="https://arxiv.org/abs/2502.14795" target="_blank"><img src="https://img.shields.io/badge/arXiv-2502.14795-B31B1B?style=flat-square"></a> Pengxiang Ding, Jianfei Ma, Xinyang Tong, Binghong Zou, Xinxin Luo, Yiguo Fan, Ting Wang, Hongchao Lu, Panzhong Mo, Jinxin Liu, Yuefan Wang, Huaicheng Zhou, Wenshuo Feng, Jiacheng Liu, <u>Siteng Huang</u>, Donglin Wang, &quot;**Humanoid-VLA: Towards Universal Humanoid Control with Visual Integration**&quot;. *arXiv preprint arXiv:2502.14795*. [[pdf](https://arxiv.org/pdf/2502.14795.pdf)]

<a href="https://arxiv.org/abs/2501.05179" target="_blank"><img src="https://img.shields.io/badge/arXiv-2501.05179-B31B1B?style=flat-square"></a> Xuyang Liu, Ziming Wang, Yuhang Han, Yingyao Wang, Jiale Yuan, Jun Song, Bo Zheng, Linfeng Zhang, <u>Siteng Huang</u>, Honggang Chen, &quot;**Global Compression Commander: Plug-and-Play Inference Acceleration for High-Resolution Large Vision-Language Models**&quot;. *arXiv preprint arXiv:2501.05179*. [[pdf](https://arxiv.org/pdf/2501.05179.pdf)] [[github](https://github.com/xuyang-liu16/GlobalCom2)]

<a href="https://arxiv.org/abs/2412.09265" target="_blank"><img src="https://img.shields.io/badge/arXiv-2412.09265-B31B1B?style=flat-square"></a> Bofang Jia, Pengxiang Ding, Can Cui, Mingyang Sun, Pengfang Qian, <u>Siteng Huang</u>, Zhaoxin Fan, Donglin Wang, &quot;**Score and Distribution Matching Policy: Advanced Accelerated Visuomotor Policies via Matched Distillation**&quot;. *arXiv preprint arXiv:2412.09265*. [[pdf](https://arxiv.org/pdf/2412.09265.pdf)] [[project page](https://sdm-policy.github.io/)]

<a href="https://arxiv.org/abs/2412.06782" target="_blank"><img src="https://img.shields.io/badge/arXiv-2412.06782-B31B1B?style=flat-square"></a> Zhefei Gong, Pengxiang Ding, Shangke Lyu, <u>Siteng Huang</u>, Mingyang Sun, Wei Zhao, Zhaoxin Fan, Donglin Wang, &quot;**CARP: Visuomotor Policy Learning via Coarse-to-Fine Autoregressive Prediction**&quot;. *arXiv preprint arXiv:2412.06782*. [[pdf](https://arxiv.org/pdf/2412.06782.pdf)] [[project page](https://carp-robot.github.io/)] [[huggingface paper](https://huggingface.co/papers/2412.06782)]

<a href="https://arxiv.org/abs/2411.17686" target="_blank"><img src="https://img.shields.io/badge/arXiv-2411.17686-B31B1B?style=flat-square"></a> Yuhang Han‚Ä†, Xuyang Liu‚Ä†, Zihan Zhang, Pengxiang Ding, Donglin Wang, Honggang Chen, Qingsen Yan, <u>Siteng Huang‚úâ</u>, &quot;**Filter, Correlate, Compress: Training-Free Token Reduction for MLLM Acceleration**&quot;. *arXiv preprint arXiv:2411.17686*. [[pdf](https://arxiv.org/pdf/2411.17686.pdf)] [[project page](https://FiCoCo-accelerate.github.io/)] [[huggingface paper](https://huggingface.co/papers/2411.17686)] [[github](https://github.com/kawhiiiileo/FiCoCo)] <a class='paper_citations_badges' data='mhpkWSYAAAAJ:KlAtU1dfN6UC' href="" target="_blank"></a>

<a href="https://arxiv.org/abs/2408.17083" target="_blank"><img src="https://img.shields.io/badge/arXiv-2408.17083-B31B1B?style=flat-square"></a> Fengyuan Dai, <u>Siteng Huang</u>, Min Zhang, Biao Gong, Donglin Wang, &quot;**Focus-Consistent Multi-Level Aggregation for Compositional Zero-Shot Learning**&quot;. *arXiv preprint arXiv:2408.17083*. [[pdf](https://arxiv.org/pdf/2408.17083.pdf)]

<a href="https://arxiv.org/abs/2405.14700" target="_blank"><img src="https://img.shields.io/badge/arXiv-2405.14700-B31B1B?style=flat-square"></a> Ting Liu‚Ä†, Xuyang Liu‚Ä†, <u>Siteng Huang</u>, Liangtao Shi, Zunnan Xu, Yi Xin, Quanjun Yin, Xiaohong Liu, &quot;**Sparse-Tuning: Adapting Vision Transformers with Efficient Fine-tuning and Inference**&quot;. *arXiv preprint arXiv:2405.14700*. [[pdf](https://arxiv.org/pdf/2405.14700.pdf)] [[github](https://github.com/liuting20/Sparse-Tuning)] <a class='paper_citations_badges' data='mhpkWSYAAAAJ:hqOjcs7Dif8C' href="" target="_blank"></a>

### Thesis

<img src="https://img.shields.io/badge/Thesis-Ph.D.-815989?style=flat-square"> <u>Siteng Huang</u>, &quot;**Model Transfer for Multimodal Understanding and Generation**&quot;. Zhejiang University, 2024.

<!-- deep green: 004e00/49846a -->

